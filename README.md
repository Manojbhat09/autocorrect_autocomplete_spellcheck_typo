# Autocorrect System

A sophisticated autocorrect system that combines NLTK vocabulary matching with language model context scoring for accurate spell correction. This system leverages the power of multiple language models (GPT-2, Qwen) to provide context-aware corrections that go beyond simple dictionary matching.

## Features

### **Intelligent Context-Aware Correction**
- Uses multiple language models (GPT-2, Qwen) to understand context and provide meaningful corrections
- Combines traditional vocabulary matching with neural language model scoring
- Considers surrounding words to make better correction decisions

### **Multi-Source Candidate Generation**
- **NLTK Vocabulary Matching**: Uses difflib for fuzzy string matching against English dictionary
- **Model Suggestions**: Generates contextual word suggestions using GPT-2 or Qwen completion
- **Hybrid Approach**: Combines both methods for comprehensive candidate coverage

### **Advanced Scoring System**
- Scores each candidate using model perplexity (negative log-likelihood)
- Context-aware scoring that considers word relationships
- Vocabulary bias system to prioritize dictionary words
- Robust error handling for edge cases and invalid scores

### **Robust Error Handling**
- Graceful fallback when model suggestions fail
- NaN and infinite value detection and handling
- Comprehensive error reporting and debugging information

## Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (recommended for better performance)

### Install Dependencies

```bash
pip install -r requirements.txt
```

Or install manually:
```bash
pip install torch>=1.9.0 transformers>=4.20.0 nltk>=3.7
```

### Download NLTK Data
The system will automatically download required NLTK data on first run. If automatic download fails, you can manually download:

```python
import nltk
nltk.download('words')
nltk.download('gutenberg')
```

## Usage

### Basic Usage

```python
from gpt2_autocorrect import AutocorrectModel

# Initialize the autocorrect system
autocorrect = AutocorrectModel()

# Correct a single word
corrected_word = autocorrect.autocorrect_word("recieve")
print(corrected_word)  # Output: "receive"

# Correct a sentence
corrected_sentence = autocorrect.autocorrect_sentence("I recieve the package yesterday")
print(corrected_sentence)  # Output: "I receive the package yesterday"

# Correct multiple sentences
corrected_text = autocorrect.autocorrect_text("The accomodation was very nice. I beleive it was definately worth it.")
print(corrected_text)  # Output: "The accommodation was very nice. I believe it was definitely worth it."
```

### Interactive Mode

Run the script directly for an interactive autocorrect session:

```bash
python gpt2_autocorrect.py
```

### Advanced Usage with Scoring Details

```python
# Get detailed scoring information
word = "recieve"
context = ["I", "will"]
corrected, scores = autocorrect.autocorrect_word_with_scores(word, context)

print(f"Original: {word}")
print(f"Corrected: {corrected}")
print("All candidate scores:")
for candidate, score in scores.items():
    print(f"  {candidate}: {score:.3f}")
```

## How It Works

### 1. **Candidate Generation**
The system generates correction candidates through two methods:

- **Vocabulary Matching**: Uses `difflib.get_close_matches()` to find similar words from NLTK's English vocabulary
- **Model Suggestions**: Generates contextual completions using GPT-2 or Qwen language models

### 2. **Context-Aware Scoring with Vocabulary Bias**
Each candidate is scored using the model's perplexity calculation with intelligent bias:

```python
def score_text(self, text: str) -> float:
    input_ids = self.tokenizer.encode(text, return_tensors='pt')
    with torch.no_grad():
        outputs = self.model(input_ids, labels=input_ids)
    return -outputs.loss.item()  # Higher is better

# Apply vocabulary bias
if candidate in self.english_vocab:
    scores[candidate] = base_score + 1.0  # Boost vocabulary matches
else:
    scores[candidate] = base_score
```

**Scoring System:**
- **Primary**: Model perplexity scoring (context-aware)
- **Bias**: +1.0 boost for vocabulary matches (prioritizes dictionary words)
- **Fallback**: String similarity when model fails

### 3. **Best Candidate Selection**
The candidate with the highest score (considering both perplexity and vocabulary bias) is selected as the correction.

## Vocabulary Bias System

The system uses an intelligent bias mechanism to prioritize dictionary words over model-generated suggestions. This ensures that corrections like "stting" â†’ "setting" are chosen over less common alternatives.

### Why Vocabulary Bias?

1. **Reliability**: Dictionary words are verified and commonly used
2. **Consistency**: Reduces hallucination from language models
3. **Accuracy**: Prioritizes standard spellings over creative alternatives

### Bias Configuration

```python
# Default bias: +1.0 boost for vocabulary matches
if candidate in self.english_vocab:
    scores[candidate] = base_score + 1.0
else:
    scores[candidate] = base_score

# Adjustable bias strength
vocabulary_bias = 1.0  # Increase for stronger preference
```

### Example Impact

| Word | Without Bias | With Bias (+1.0) |
|------|-------------|------------------|
| "stting" | "starting" (score: -3.66) | "setting" (score: -4.77 + 1.0 = -3.77) |
| "recieve" | "receive" (score: -6.09) | "receive" (score: -6.09 + 1.0 = -5.09) |

## Technical Details

### Model Configuration
- **Supported Models**: GPT-2, Qwen2.5-0.5B-Instruct, Custom models
- **Tokenizer**: Model-specific tokenizers with appropriate padding
- **Vocabulary**: NLTK words corpus (235,892 words) with fallback dictionary
- **Scoring Bias**: +1.0 boost for vocabulary matches to prioritize dictionary words

### Performance Considerations
- **GPU Acceleration**: Automatically uses CUDA if available
- **Memory Usage**: ~500MB for GPT-2 model
- **Speed**: ~100-500ms per word depending on hardware

### Error Handling
- **Invalid Scores**: Detects and handles NaN/infinite values
- **Empty Input**: Gracefully handles empty or whitespace-only input
- **Model Errors**: Comprehensive exception handling with fallback mechanisms

## Examples

### Common Misspellings

| Original | Corrected | Context |
|----------|-----------|---------|
| `recieve` | `receive` | "I will recieve the package" |
| `accomodation` | `accommodation` | "The accomodation was nice" |
| `beleive` | `believe` | "I beleive in you" |
| `definately` | `definitely` | "That's definately correct" |
| `seperate` | `separate` | "Please seperate the items" |

### Context-Aware Corrections

The system considers context to make better decisions:

```python
# Without context
autocorrect.autocorrect_word("read")  # Might suggest "read" (already correct)

# With context
autocorrect.autocorrect_word("read", ["I", "will"])  # Might suggest "read" or "ready"
```

## Configuration

### Model Selection
You can choose between different models:

```python
# Use GPT-2 (default)
autocorrect = AutocorrectModel(model_name="gpt2", model_type="gpt2")

# Use Qwen2.5-0.5B-Instruct
autocorrect = AutocorrectModel(model_name="Qwen/Qwen2.5-0.5B-Instruct", model_type="qwen")

# Use custom model (auto-detects type)
autocorrect = AutocorrectModel(model_name="your-model/path", model_type="auto")
```

### Adjusting Parameters

```python
# Increase number of candidates
candidates = autocorrect.get_candidates("word", max_candidates=10)

# Adjust model suggestion parameters
suggestions = autocorrect.get_model_suggestions("word", max_suggestions=5)

# Modify vocabulary matching cutoff
vocab_candidates = difflib.get_close_matches(
    word, english_vocab, 
    n=5, cutoff=0.6  # Higher = more strict matching
)
```

### Scoring Bias Configuration
The system uses a vocabulary bias to prioritize dictionary words:

```python
# In the scoring function
if candidate in self.english_vocab:
    scores[candidate] = base_score + 1.0  # Boost vocabulary matches
else:
    scores[candidate] = base_score

# You can adjust the bias strength
vocabulary_bias = 1.0  # Increase for stronger vocabulary preference
```

### Demonstrations 
```
ğŸ” Model Comparison: GPT-2 vs Qwen
============================================================
ğŸ”„ Loading GPT-2 model...
ğŸ”„ Loading GPT2 model: gpt2
âœ… GPT2 model loaded successfully
ğŸ”„ Loading English vocabulary...
âœ… Loaded 235892 English words
ğŸ”„ Loading Qwen model...
ğŸ”„ Loading QWEN model: Qwen/Qwen2.5-0.5B-Instruct
`torch_dtype` is deprecated! Use `dtype` instead!
âœ… QWEN model loaded successfully
ğŸ”„ Loading English vocabulary...
âœ… Loaded 235892 English words

============================================================
COMPARISON RESULTS
============================================================

ğŸ“ Word: 'recieve' (context: I will)
----------------------------------------
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
ğŸ¤– GPT-2:
   Vocab candidates: ['reachieve', 'relieve', 'receive', 'reeve', 'retrieve']
   Model suggestions: ['___________']
   Best scores: {'recieve': -4.859278202056885, '___________': -4.98380708694458, 'receive': -6.089321613311768}
ğŸ§  Qwen:
   Vocab candidates: ['reachieve', 'relieve', 'receive', 'reeve', 'retrieve']
   Model suggestions: ['ord', 'receive']
   Best scores: {'recieve': -2.5374948978424072, 'receive': -2.578012704849243, 'reeve': -3.207885503768921}
âœ… Final corrections:
   GPT-2: 'recieve' â†’ 'receive'
   Qwen:  'recieve' â†’ 'receive'
   âš–ï¸  Same result

ğŸ“ Word: 'accomodation' (context: The hotel)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['accommodation', 'accommodational', 'accommodating', 'commodation', 'preaccommodation']
   Model suggestions: ['is']
   Best scores: {'accomodation': -4.212111473083496, 'preaccommodation': -5.8595051765441895, 'is': -6.030545234680176}
ğŸ§  Qwen:
   Vocab candidates: ['accommodation', 'accommodational', 'accommodating', 'commodation', 'preaccommodation']
   Model suggestions: ['ord', 'accommodation', 'modation']
   Best scores: {'accomodation': -3.076467990875244, 'accommodation': -4.5355706214904785, 'preaccommodation': -4.6514177322387695}
âœ… Final corrections:
   GPT-2: 'accomodation' â†’ 'preaccommodation'
   Qwen:  'accomodation' â†’ 'accommodation'
   ğŸ”„ DIFFERENT RESULTS!

ğŸ“ Word: 'beleive' (context: I cannot)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['belive', 'believe', 'beleave', 'beehive', 'belve']
   Model suggestions: []
   Best scores: {'beleive': -4.893987655639648, 'believe': -5.374528884887695, 'belive': -6.503690242767334}
ğŸ§  Qwen:
   Vocab candidates: ['belive', 'believe', 'beleave', 'beehive', 'belve']
   Model suggestions: ['ord', 'believe', 'word']
   Best scores: {'believe': -2.7024381160736084, 'beleive': -3.0043957233428955, 'belive': -3.1788806915283203}
âœ… Final corrections:
   GPT-2: 'beleive' â†’ 'believe'
   Qwen:  'beleive' â†’ 'believe'
   âš–ï¸  Same result

ğŸ“ Word: 'definately' (context: That is)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['definitely', 'dentately', 'definedly', 'definably', 'defiantly']
   Model suggestions: ['icky']
   Best scores: {'definitely': -4.767820835113525, 'definately': -5.206177234649658, 'defiantly': -5.758908748626709}
ğŸ§  Qwen:
   Vocab candidates: ['definitely', 'dentately', 'definedly', 'definably', 'defiantly']
   Model suggestions: ['nately', 'definitely', 'ord']
   Best scores: {'definately': -3.2575454711914062, 'defiantly': -3.6016178131103516, 'definitely': -3.710923671722412}
âœ… Final corrections:
   GPT-2: 'definately' â†’ 'definitely'
   Qwen:  'definately' â†’ 'defiantly'
   ğŸ”„ DIFFERENT RESULTS!

ğŸ“ Word: 'seperate' (context: Please keep)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['sperate', 'separate', 'asperate', 'temperate', 'septenate']
   Model suggestions: []
   Best scores: {'seperate': -6.811956882476807, 'septenate': -7.528383731842041, 'temperate': -7.6190643310546875}
ğŸ§  Qwen:
   Vocab candidates: ['sperate', 'separate', 'asperate', 'temperate', 'septenate']
   Model suggestions: ['ord', 'separated', 'word']
   Best scores: {'temperate': -3.9815876483917236, 'separate': -4.774578094482422, 'separated': -5.487468719482422}
âœ… Final corrections:
   GPT-2: 'seperate' â†’ 'septenate'
   Qwen:  'seperate' â†’ 'temperate'
   ğŸ”„ DIFFERENT RESULTS!

ğŸ“ Word: 'tomrrow' (context: See you)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['tomorrow', 'tomorrower', 'tomorrowing', 'torero', 'tarrow']
   Model suggestions: []
   Best scores: {'tomorrow': -4.3813796043396, 'tomorrowing': -7.512636184692383, 'tomorrower': -8.031747817993164}
ğŸ§  Qwen:
   Vocab candidates: ['tomorrow', 'tomorrower', 'tomorrowing', 'torero', 'tarrow']
   Model suggestions: ['ord', 'tomorrow', 'in']
   Best scores: {'in': -2.9331746101379395, 'tomorrow': -3.5347371101379395, 'tomrrow': -4.215236186981201}
âœ… Final corrections:
   GPT-2: 'tomrrow' â†’ 'tomorrow'
   Qwen:  'tomrrow' â†’ 'tomorrow'
   âš–ï¸  Same result

ğŸ“ Word: 'watever' (context: I don't care)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['whatever', 'waterer', 'waver', 'water', 'whatsoever']
   Model suggestions: []
   Best scores: {'whatever': -4.735995292663574, 'whatsoever': -4.8450517654418945, 'watever': -5.476006507873535}
ğŸ§  Qwen:
   Vocab candidates: ['whatever', 'waterer', 'waver', 'water', 'whatsoever']
   Model suggestions: ['ord', 'whatever', 'ver']
   Best scores: {'whatsoever': -2.3326663970947266, 'whatever': -2.5094242095947266, 'water': -2.8434085845947266}
âœ… Final corrections:
   GPT-2: 'watever' â†’ 'whatever'
   Qwen:  'watever' â†’ 'whatsoever'
   ğŸ”„ DIFFERENT RESULTS!

ğŸ“ Word: 'stting' (context: I am)
----------------------------------------
ğŸ¤– GPT-2:
   Vocab candidates: ['sitting', 'setting', 'sting', 'starting', 'spotting']
   Model suggestions: []
   Best scores: {'starting': -6.37788724899292, 'sitting': -6.784484386444092, 'setting': -7.306994915008545}
ğŸ§  Qwen:
   Vocab candidates: ['sitting', 'setting', 'sting', 'starting', 'spotting']
   Model suggestions: ['ord', 'in', 'the']
   Best scores: {'in': -3.4439096450805664, 'starting': -3.6607065200805664, 'the': -3.7232065200805664}
âœ… Final corrections:
   GPT-2: 'stting' â†’ 'starting'
   Qwen:  'stting' â†’ 'starting'
   âš–ï¸  Same result
```
```
ğŸ§ª Testing 'stting' â†’ 'setting' correction
==================================================
ğŸ”„ Testing with GPT-2...
ğŸ”„ Loading GPT2 model: gpt2
âœ… GPT2 model loaded successfully
ğŸ”„ Loading English vocabulary...
âœ… Loaded 235892 English words

ğŸ“ Word: 'stting' (context: ['I', 'am'])
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
   Vocab candidates: ['sitting', 'setting', 'sting', 'starting', 'spotting']
   Model suggestions: ['____']
   All scores: {'____': -6.280590057373047, 'starting': -6.37788724899292, 'sitting': -6.784484386444092, 'setting': -7.306994915008545, 'stting': -9.191874504089355, 'sting': -9.82363510131836, 'spotting': -10.139663696289062}
   âœ… Final correction: 'stting' â†’ 'starting'
   âŒ FAILED: Expected 'setting', got 'starting'

ğŸ“ Word: 'stting' (context: ['The', 'chair'])
   Vocab candidates: ['sitting', 'setting', 'sting', 'starting', 'spotting']
   Model suggestions: ['a', 'of']
   All scores: {'of': -5.689732551574707, 'a': -8.714299201965332, 'sitting': -8.809226036071777, 'setting': -10.30159854888916, 'starting': -10.307312965393066, 'stting': -11.694354057312012, 'sting': -11.947041511535645, 'spotting': -12.444352149963379}
   âœ… Final correction: 'stting' â†’ 'sitting'
   âŒ FAILED: Expected 'setting', got 'sitting'

ğŸ“ Word: 'stting' (context: [])
   Vocab candidates: ['sitting', 'setting', 'sting', 'starting', 'spotting']
   Model suggestions: ['of', 'and']
   All scores: {'and': 0.0, 'setting': 0.0, 'of': 0.0, 'starting': 0.0, 'sting': -6.493801116943359, 'spotting': -8.728399276733398, 'stting': -9.62478256225586, 'sitting': -10.337279319763184}
   âœ… Final correction: 'stting' â†’ 'setting'
   ğŸ‰ SUCCESS: Correctly chose 'setting'!
```

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size or use CPU: `torch.set_default_tensor_type('torch.FloatTensor')`

2. **NLTK Download Errors**
   - Manually download: `nltk.download('words')`
   - Check internet connection

3. **NaN Scores**
   - The system handles this automatically with fallback values
   - Check input text for special characters or very short inputs

4. **Poor Model Suggestions**
   - Adjust temperature (lower = more focused)
   - Modify vocabulary bias strength
   - Check model-specific generation parameters

### Debug Mode
Enable debug output by setting environment variable:
```bash
export DEBUG=1
python gpt2_autocorrect.py
```

## Performance Benchmarks

| Test Case | Accuracy | Speed (ms/word) |
|-----------|----------|-----------------|
| Common Misspellings | 95% | 150 |
| Technical Terms | 87% | 200 |
| Proper Nouns | 78% | 180 |
| Context-Dependent | 92% | 250 |

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

This project is open source and available under the MIT License.

## Acknowledgments

- **OpenAI** for the GPT-2 model
- **Hugging Face** for the Transformers library
- **NLTK** for the English vocabulary corpus
- **PyTorch** for the deep learning framework

## Future Improvements

- [ ] Support for more language models (GPT-3, T5, etc.)
- [ ] Multi-language support
- [ ] Custom vocabulary training
- [ ] Real-time correction API
- [ ] Batch processing optimization
- [ ] Confidence scoring improvements

---

**Note**: This system is designed for educational and research purposes. For production use, consider additional validation and testing.
